{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python.exe -m pip install --upgrade pip\n",
    "# pip install ipykernel\n",
    "# pip install jupyter\n",
    "# python -m pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui\n",
    "# pip install langchain einops accelerate transformers scipy xformers\n",
    "# pip install torch==2.0.1+cu117 torchvision==0.15.2+cu117 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# pip install sentencepiece sentence_transformers\n",
    "# pip install llama-index\n",
    "# pip install llama-hub\n",
    "# pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tranformer classes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from transformers import BitsAndBytesConfig\n",
    "# Import torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name\n",
    "name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# name = \"togethercomputer/LLaMA-2-7B-32K\"\n",
    "# Set auth variable from hugging face\n",
    "auth_token = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Git Repo Local Archive\\GAN Poetry\\project\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:655: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "d:\\Git Repo Local Archive\\GAN Poetry\\project\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\Git Repo Local Archive\\GAN Poetry\\model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    name,\n",
    "    cache_dir=\"./model/\",\n",
    "    use_auth_token=auth_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Git Repo Local Archive\\GAN Poetry\\project\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712e7c17f12d4ca9887ca3a30fb635ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin d:\\Git Repo Local Archive\\GAN Poetry\\project\\Lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1760b95f5ed5453980235fce22009299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Git Repo Local Archive\\GAN Poetry\\project\\Lib\\site-packages\\transformers\\utils\\hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name,\n",
    "    cache_dir=\"./model/\",\n",
    "    use_auth_token=auth_token,\n",
    "    rope_scaling={\"type\":\"dynamic\",\"factor\":2},\n",
    "    load_in_4bit=True,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup prompt\n",
    "prompt = \"### User:What is the fastest car in the world and how much does it cost? ### Assistant:\"\n",
    "# Pass the prompt to the tokenizer\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "# Setup text streamer\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fastest car in the world is subjective and can vary depending on the criteria used to measure speed. Unterscheidung between a sports car and a supercar. A sports car is typically designed for speed and handling on a racetrack, while a supercar is a high-performance vehicle with luxury features and amenities.\n",
      "\n",
      "Some of the fastest cars in the world include:\n",
      "\n",
      "1. Bugatti Chiron Super Sport - 300+ mph (483+ km/h) - $3 million+\n",
      "2. Hennessey Venom F5 - 301+ mph (485+ km/h) - $1.6 million+\n",
      "3. Koenigsegg Regera - 248+ mph (399+ km/h) - $2.5 million+\n",
      "4. Pagani Huayra BC - 238+ mph (383+ km/h) - $2.8 million+\n",
      "5. Rimac C_Two - 258+ mph (415+ km/h) - $1.9 million+\n",
      "\n",
      "Note: These prices are approximate and can vary depending on location, taxes, and other factors.\n",
      "\n",
      "It's important to note that these cars are not only fast but also very expensive, and their prices can vary greatly depending on the brand, model, and other factors. Additionally, the top speed of a car is not the only factor to consider when evaluating its performance, as other factors such as acceleration, handling, and braking performance are also important.</s>\n"
     ]
    }
   ],
   "source": [
    "# Actually run the thing\n",
    "output = model.generate(inputs[\"input_ids\"], streamer=streamer, use_cache=True, max_new_tokens=float(\"inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the output tokens back to text\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### User:What is the fastest car in the world and how much does it cost? ### Assistant:The fastest car in the world is subjective and can vary depending on the criteria used to measure speed. Unterscheidung between a sports car and a supercar. A sports car is typically designed for speed and handling on a racetrack, while a supercar is a high-performance vehicle with luxury features and amenities.\n",
      "\n",
      "Some of the fastest cars in the world include:\n",
      "\n",
      "1. Bugatti Chiron Super Sport - 300+ mph (483+ km/h) - $3 million+\n",
      "2. Hennessey Venom F5 - 301+ mph (485+ km/h) - $1.6 million+\n",
      "3. Koenigsegg Regera - 248+ mph (399+ km/h) - $2.5 million+\n",
      "4. Pagani Huayra BC - 238+ mph (383+ km/h) - $2.8 million+\n",
      "5. Rimac C_Two - 258+ mph (415+ km/h) - $1.9 million+\n",
      "\n",
      "Note: These prices are approximate and can vary depending on location, taxes, and other factors.\n",
      "\n",
      "It's important to note that these cars are not only fast but also very expensive, and their prices can vary greatly depending on the brand, model, and other factors. Additionally, the top speed of a car is not the only factor to consider when evaluating its performance, as other factors such as acceleration, handling, and braking performance are also important.\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the prompt wrapper...but for llama index\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "# Create a system prompt\n",
    "system_prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as \n",
    "helpfully as possible, while being safe. Your answers should not include\n",
    "any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain \n",
    "why instead of answering something not correct. If you don't know the answer \n",
    "to a question, please don't share false information.\n",
    "\n",
    "Your goal is to provide answers relating to the Constitution of India.<</SYS>>\"\"\"\n",
    "# Throw together the query wrapper\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"{query_str} [/INST]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello [/INST]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compelete the query prompt\n",
    "query_wrapper_prompt.format(query_str='hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the llama index HF Wrapper\n",
    "from llama_index.llms import HuggingFaceLLM\n",
    "# Create a HF LLM using the llama index wrapper\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=1024,\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in embeddings wrapper\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "# Bring in HF embeddings - need these to represent document chunks\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b518615cd2480896c380ab09d18638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57d1f7ee7c04b31ab0fd2c8693a602d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868daa27c0dc4c9fb388c918ff2c3140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f0cdfb935146678f675462912253cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429b68a51b0b4fb699061cce0f9cf3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381dcf7a253f4911a78d6531163c5447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560df11fb2354df0bdb0a36654bf0c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c91b72667241d4931f2f00d90646a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2bd357ed014b3bbde4a9f525d8f8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e64650f011f4c1891dde1bda8af47b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dbb6781f9af4eac8d99d3bf0e2006ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0957a128d14caba25330bc6c01a3ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8748f282c7f436883d9943dc74b12ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ca6e968ea4419fad6cc4dff4962ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an Embeddings instance\n",
    "embeddings = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in stuff to change service context\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sagar\\AppData\\Local\\llama_index...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# Create new service context instance\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    chunk_size=1024,\n",
    "    llm=llm,\n",
    "    embed_model=embeddings\n",
    ")\n",
    "# And set the service context\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classes to load documents\n",
    "from llama_index import VectorStoreIndex, download_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download PDF loader\n",
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")\n",
    "# Create PDF loader\n",
    "loader = PyMuPDFReader()\n",
    "# Load documents\n",
    "documents = loader.load(file_path=r\"D:\\Git Repo Local Archive\\GAN Poetry\\10_Two-stage ELM for phishing Web pages detection using.pdf\",metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index - we'll be able to query this in a sec\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup index query engine using LLM\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Test out a query in Natural language\n",
    "response = query_engine.query(\"Summarize this article in 2000 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The article discusses the problem of phishing on the internet and the need for an efficient method for detecting phishing websites. The authors propose a two-stage framework using Extreme Learning Machine (ELM) for phishing detection, which combines textual content features and rule-based features.\n",
      "\n",
      "The first stage of the framework involves using ELM to construct a textual content classification model, which predicts the labels of textual contents. The authors propose nine basic features, including the URL, IP address, domain name, and MX record, to construct the textual content features. They also introduce a rule-based feature that represents the relationship between internal and external links on a website.\n",
      "\n",
      "In the second stage, the authors utilize a linear combination model-based ensemble ELM (LC-ELM) to construct a detection module on the hybrid features. The hybrid features include the textual content labels predicted by the ELM model and the rule-based feature. The LC-ELM model combines the predictions of multiple base classifiers to improve the accuracy of phishing detection.\n",
      "\n",
      "The authors evaluate the proposed framework using a dataset of phishing and legitimate websites. The results show that the two-stage ELM framework with hybrid features outperforms single ELM and other machine learning algorithms in terms of accuracy and efficiency.\n",
      "\n",
      "The main contributions of the paper can be summarized as follows:\n",
      "\n",
      "1. Propose a two-stage framework for phishing detection using ELM and hybrid features.\n",
      "2. Introduce a rule-based feature that represents the relationship between internal and external links on a website.\n",
      "3. Utilize a linear combination model-based ensemble ELM (LC-ELM) to construct a detection module on hybrid features.\n",
      "4. Evaluate the proposed framework using a dataset of phishing and legitimate websites.\n",
      "\n",
      "Overall, the paper provides a novel approach to phishing detection using ELM and hybrid features, which can improve the accuracy and efficiency of phishing detection systems.\n"
     ]
    }
   ],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
